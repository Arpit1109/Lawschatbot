{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import langchain\n",
    "import faiss\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import faiss\n",
    "\n",
    "import PyPDF2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPEN_API_KEY=\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Pinecone\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "from pinecone import Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = []\n",
    "    with open(pdf_path, \"rb\") as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        num_pages = len(reader.pages)\n",
    "        print(\"NUM Pages:\",num_pages)\n",
    "        for page in range(num_pages):\n",
    "            pdf_page = reader.pages[page]\n",
    "            text.append(pdf_page.extract_text())\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embeddings_creation(data):\n",
    "        chunks=data\n",
    "        # List to store the embeddings\n",
    "        # Generate embeddings for each chunk\n",
    "        for chunk in chunks:\n",
    "            paragraph.append(chunk)\n",
    "            response = embedding.embed_query(chunk)\n",
    "            embeddings.append(response)\n",
    "        return embeddings, paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'openai' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mopenai\u001b[49m\u001b[38;5;241m.\u001b[39mapi_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      5\u001b[0m embedding \u001b[38;5;241m=\u001b[39m SentenceTransformerEmbeddings(model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall-MiniLM-L6-v2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'openai' is not defined"
     ]
    }
   ],
   "source": [
    "openai.api_key=\"\"\n",
    "\n",
    "\n",
    "\n",
    "embedding = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = []\n",
    "paragraph = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embeddings_to_indexer(embeddings):\n",
    "        nlist = 1\n",
    "        # Convert the embeddings to a numpy array\n",
    "        embeddings = np.array(embeddings)\n",
    "        # Set the dimension of the embeddings\n",
    "        embedding_dim = embeddings.shape[1]\n",
    "        print(\"Embedding Dimension:\",embedding_dim)\n",
    "        # print(\"Embedding Dimension:\",embedding_dim)\n",
    "        # Initialize the index\n",
    "        # Add the embeddings to the index\n",
    "        quantiser = faiss.IndexFlatL2(embedding_dim)\n",
    "        indexer = faiss.IndexIVFFlat(quantiser, embedding_dim, nlist,   faiss.METRIC_L2)\n",
    "        \n",
    "        #train the model\n",
    "        indexer.train(embeddings)\n",
    "        indexer.add(embeddings)\n",
    "        return indexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def question_query(question):\n",
    "        query_vector = embedding.embed_query(question)\n",
    "        return query_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_answering_model(query_vector, paragraph,indexer):\n",
    "    embedd = np.array(query_vector)\n",
    "    embed_reshaped = embedd.reshape(1, 384)\n",
    "    distances, indices = indexer.search(embed_reshaped,k=2)\n",
    "    relevant_documents = ''\n",
    "\n",
    "    for index in indices[0]:\n",
    "        # Replace this with your actual code to retrieve the relevant documents based on the index\n",
    "        document = paragraph[index]\n",
    "        relevant_documents += (document + \" \")\n",
    "\n",
    "    print(relevant_documents)\n",
    "    return relevant_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prompt(user_query, relevant_documents):\n",
    "  escaped = relevant_documents.replace(\"'\", \"\").replace('\"', \"\").replace(\"\\n\", \" \")\n",
    "  prompt = textwrap.dedent(\"\"\"You are a helpful and informative bot that answers questions using text from the reference passage included below. \\\n",
    "  Be sure to respond in a complete sentence, being comprehensive, including all relevant background information. \\\n",
    "  However, you are talking to a non-technical audience, so be sure to break down complicated concepts and \\\n",
    "  strike a friendly and converstional tone. \\\n",
    "  If the passage is irrelevant to the answer, you may ignore it.\n",
    "  QUESTION: '{user_query}'\n",
    "  PASSAGE: '{relevant_documents}'\n",
    "\n",
    "    ANSWER:\n",
    "  \"\"\").format(user_query=user_query, relevant_documents=escaped)\n",
    "\n",
    "  return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = os.path.join(\"data\", \"lawsofpower.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUM Pages: 17\n",
      "34 34\n",
      "Embedding Dimension: 384\n",
      "CREATED\n"
     ]
    }
   ],
   "source": [
    "data = extract_text_from_pdf(pdf_path)\n",
    "embeddings, paragraph = embeddings_creation(data)\n",
    "print(len(embeddings),len(paragraph))\n",
    "faiss_indexer = embeddings_to_indexer(embeddings)\n",
    "print(\"CREATED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response =openai.chat.completions.create(\n",
    "             model=\"gpt-3.5-turbo\",\n",
    "             messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    ")\n",
    "    return response.choices[0].message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(question):\n",
    "    #  embeddings, paragraph = embeddings_creation(data)\n",
    "     \n",
    "     query_vector = question_query(question)\n",
    "     relevant_documents = question_answering_model(query_vector, paragraph, indexer=faiss_indexer)\n",
    "     prompt = make_prompt(query_vector,relevant_documents)\n",
    "     answer=get_completion(prompt)\n",
    "     return Markdown(answer.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_vector = question_query(\"Can you tell me the story of Queen Elizabeth I from this 48 laws of power book?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      " \n",
      "The 48 Laws Of Power \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "By  \n",
      "Robert Greene \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Penguin Books 2000 \n",
      "Hardcover Edition ISBN 0-670-88146-5 \n",
      "Paperback Edition ISBN 0 14 02.8019 7 \n",
      "452 pages \n",
      " \n",
      " \n",
      "WISDOM IN A NUTSHELL  \n",
      " \n",
      "The 48 Laws Of Power \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "By  \n",
      "Robert Greene \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Penguin Books 2000 \n",
      "Hardcover Edition ISBN 0-670-88146-5 \n",
      "Paperback Edition ISBN 0 14 02.8019 7 \n",
      "452 pages \n",
      " \n",
      " \n",
      "WISDOM IN A NUTSHELL \n"
     ]
    }
   ],
   "source": [
    "relevant_documents = question_answering_model(query_vector, paragraph, indexer=faiss_indexer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
